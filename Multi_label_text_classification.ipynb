{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avniiii2606/Email-Classification-using-BERT-LDA/blob/main/Multi_label_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a1ywYioe3fp9",
        "outputId": "4f2a03c3-48c9-4952-ee5e-963d0fbbcf69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qal-FUSZ4qkn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    hamming_loss,\n",
        "    multilabel_confusion_matrix\n",
        ")\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m_2j4HSUAsba"
      },
      "outputs": [],
      "source": [
        "def load_and_prepare_data(path):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Convert string representations of lists to actual lists\n",
        "    df['text'] = df['ABSTRACT']+df['TITLE']\n",
        "    df['labels'] =  df[['Computer Science', 'Physics', 'Mathematics', 'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values.tolist()\n",
        "\n",
        "    # Transform labels to binary format\n",
        "    mlb = MultiLabelBinarizer()\n",
        "    labels = mlb.fit_transform(df['labels'])\n",
        "    label_names = mlb.classes_.tolist()  # Convert NumPy array to list\n",
        "    label_names = [str(label) for label in label_names] # Convert each element to string\n",
        "\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['text'],\n",
        "        labels,\n",
        "        test_size=0.2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, mlb.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QHevVPcLAuIB"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(y_true, y_pred, label_names):\n",
        "    \"\"\"\n",
        "    Comprehensive evaluation of the multi-label classification model.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    y_true : array-like\n",
        "        True labels\n",
        "    y_pred : array-like\n",
        "        Predicted labels\n",
        "    label_names : array-like\n",
        "        Names of the labels\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Dictionary containing all evaluation metrics\n",
        "    \"\"\"\n",
        "    # Calculate overall metrics\n",
        "    metrics = {\n",
        "        'hamming_loss': hamming_loss(y_true, y_pred),\n",
        "        'accuracy_score': accuracy_score(y_true, y_pred),\n",
        "        'micro_precision': precision_score(y_true, y_pred, average='micro'),\n",
        "        'macro_precision': precision_score(y_true, y_pred, average='macro'),\n",
        "        'weighted_precision': precision_score(y_true, y_pred, average='weighted'),\n",
        "        'micro_recall': recall_score(y_true, y_pred, average='micro'),\n",
        "        'macro_recall': recall_score(y_true, y_pred, average='macro'),\n",
        "        'weighted_recall': recall_score(y_true, y_pred, average='weighted'),\n",
        "        'micro_f1': f1_score(y_true, y_pred, average='micro'),\n",
        "        'macro_f1': f1_score(y_true, y_pred, average='macro'),\n",
        "        'weighted_f1': f1_score(y_true, y_pred, average='weighted')\n",
        "    }\n",
        "\n",
        "    # Calculate per-label metrics\n",
        "    per_label_metrics = {}\n",
        "    for i, label in enumerate(label_names):\n",
        "        per_label_metrics[label] = {\n",
        "            'precision': precision_score(y_true[:, i], y_pred[:, i]),\n",
        "            'recall': recall_score(y_true[:, i], y_pred[:, i]),\n",
        "            'f1': f1_score(y_true[:, i], y_pred[:, i])\n",
        "        }\n",
        "\n",
        "    # Generate confusion matrices\n",
        "    confusion_matrices = multilabel_confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Print detailed evaluation report\n",
        "    print(\"\\n=== Model Evaluation Report ===\")\n",
        "    print(\"\\nOverall Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    print(\"\\nPer-label Metrics:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=label_names))\n",
        "\n",
        "    # Visualize confusion matrices\n",
        "    plot_confusion_matrices(confusion_matrices, label_names)\n",
        "\n",
        "    # Plot label distribution\n",
        "    plot_label_distribution(y_true, y_pred, label_names)\n",
        "\n",
        "    return {\n",
        "        'overall_metrics': metrics,\n",
        "        'per_label_metrics': per_label_metrics,\n",
        "        'confusion_matrices': confusion_matrices\n",
        "    }\n",
        "\n",
        "def plot_confusion_matrices(confusion_matrices, label_names):\n",
        "    \"\"\"\n",
        "    Plot confusion matrices for each label.\n",
        "    \"\"\"\n",
        "    n_labels = len(label_names)\n",
        "    fig, axes = plt.subplots(2, (n_labels + 1) // 2, figsize=(15, 8))\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    for i, (matrix, label) in enumerate(zip(confusion_matrices, label_names)):\n",
        "        sns.heatmap(matrix, annot=True, fmt='d', ax=axes[i])\n",
        "        axes[i].set_title(f'Confusion Matrix: {label}')\n",
        "        axes[i].set_xlabel('Predicted')\n",
        "        axes[i].set_ylabel('True')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_label_distribution(y_true, y_pred, label_names):\n",
        "    \"\"\"\n",
        "    Plot the distribution of true vs predicted labels.\n",
        "    \"\"\"\n",
        "    true_dist = y_true.sum(axis=0)\n",
        "    pred_dist = y_pred.sum(axis=0)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    x = np.arange(len(label_names))\n",
        "    width = 0.35\n",
        "\n",
        "    plt.bar(x - width/2, true_dist, width, label='True')\n",
        "    plt.bar(x + width/2, pred_dist, width, label='Predicted')\n",
        "\n",
        "    plt.xlabel('Labels')\n",
        "    plt.ylabel('Count')\n",
        "    plt.title('Label Distribution: True vs Predicted')\n",
        "    plt.xticks(x, label_names, rotation=45, ha='right')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vv-TsdVQA5wt"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_lda(X_train, X_test, y_train, y_test, label_names):\n",
        "    # Convert text to TF-IDF features\n",
        "    vectorizer = TfidfVectorizer(max_features=5000)\n",
        "    X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "    X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "    # Train one LDA for each label\n",
        "    predictions = []\n",
        "    for i in range(y_train.shape[1]):\n",
        "        # Train LDA for current label\n",
        "        lda = LinearDiscriminantAnalysis()\n",
        "        lda.fit(X_train_tfidf.toarray(), y_train[:, i])\n",
        "\n",
        "        # Make predictions for current label\n",
        "        pred = lda.predict(X_test_tfidf.toarray())\n",
        "        predictions.append(pred)\n",
        "\n",
        "    # Combine predictions for all labels\n",
        "    y_pred = np.array(predictions).T\n",
        "\n",
        "    # Evaluate the model\n",
        "    evaluation_results = evaluate_model(y_test, y_pred, label_names)\n",
        "\n",
        "    return y_pred, evaluation_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "GWhWLSRuA_rh",
        "outputId": "091723d7-8b1e-437f-bd35-eebbfd67c98a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Model Evaluation Report ===\n",
            "\n",
            "Overall Metrics:\n",
            "hamming_loss: 0.0000\n",
            "accuracy_score: 1.0000\n",
            "micro_precision: 1.0000\n",
            "macro_precision: 1.0000\n",
            "weighted_precision: 1.0000\n",
            "micro_recall: 1.0000\n",
            "macro_recall: 1.0000\n",
            "weighted_recall: 1.0000\n",
            "micro_f1: 1.0000\n",
            "macro_f1: 1.0000\n",
            "weighted_f1: 1.0000\n",
            "\n",
            "Per-label Metrics:\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "object of type 'numpy.int64' has no len()",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-3453c0b117b3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-3453c0b117b3>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Train model, get predictions and evaluation results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     predictions, evaluation_results = train_and_evaluate_lda(\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     )\n",
            "\u001b[0;32m<ipython-input-8-c0898b63f238>\u001b[0m in \u001b[0;36mtrain_and_evaluate_lda\u001b[0;34m(X_train, X_test, y_train, y_test, label_names)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mevaluation_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluation_results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-3843b8a3a147>\u001b[0m in \u001b[0;36mevaluate_model\u001b[0;34m(y_true, y_pred, label_names)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nPer-label Metrics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;31m# Visualize confusion matrices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2722\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m         \u001b[0mlongest_last_line_heading\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"weighted avg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2724\u001b[0;31m         \u001b[0mname_width\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2725\u001b[0m         \u001b[0mwidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlongest_last_line_heading\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdigits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2726\u001b[0m         \u001b[0mhead_fmt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{:>{width}s} \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" {:>9}\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int64' has no len()"
          ]
        }
      ],
      "source": [
        "def main():\n",
        "    # Load and prepare data\n",
        "    path=\"/content/drive/MyDrive/multi_label_text.csv\"\n",
        "    X_train, X_test, y_train, y_test, label_names = load_and_prepare_data(path)\n",
        "\n",
        "    # Train model, get predictions and evaluation results\n",
        "    predictions, evaluation_results = train_and_evaluate_lda(\n",
        "        X_train, X_test, y_train, y_test, label_names\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilAptiUXBFbM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyObRS4W/hnVHsd0X7DNKB48",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}