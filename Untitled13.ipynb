{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNK3FQnm02GfDkUjCxiy7kL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avniiii2606/Email-Classification-using-BERT-LDA/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rho2gSJyNzt",
        "outputId": "a5c6248d-1bcd-4aa3-d715-628bbe02e251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd\n",
        "!ls /content/drive/MyDrive/spam_email_dataset.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "040i7SEkzyui",
        "outputId": "72f23cec-3496-46c3-a3be-cfa9559b1087"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "/content/drive/MyDrive/spam_email_dataset.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, bert_model=\"bert-base-uncased\", num_classes=5):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained(bert_model)\n",
        "\n",
        "        # Neural network layers after BERT\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),  # Increased dropout for better regularization\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "class EmailDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "def prepare_data(df):\n",
        "    \"\"\"Prepare and preprocess the email data\"\"\"\n",
        "    # Clean the text data\n",
        "    df['text'] = df['text'].str.lower()\n",
        "    df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n",
        "    df['text'] = df['text'].str.strip()\n",
        "\n",
        "    # Convert categories to numerical labels\n",
        "    le = LabelEncoder()\n",
        "    df['label'] = le.fit_transform(df['label'])\n",
        "\n",
        "    return df, le.classes_\n",
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=5, learning_rate=2e-5):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        train_bar = tqdm(train_loader, desc=f'Training Epoch {epoch+1}/{num_epochs}')\n",
        "        for batch in train_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_bar.set_postfix({'loss': loss.item(),\n",
        "                                 'accuracy': 100. * train_correct / train_total})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            val_bar = tqdm(val_loader, desc='Validation')\n",
        "            for batch in val_bar:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "                val_bar.set_postfix({'loss': loss.item(),\n",
        "                                   'accuracy': 100. * val_correct / val_total})\n",
        "\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Save best model\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_model_state = model.state_dict().copy()\n",
        "\n",
        "        print(f'\\nEpoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Training Loss: {total_loss/len(train_loader):.4f}')\n",
        "        print(f'Training Accuracy: {100 * train_correct/train_total:.2f}%')\n",
        "        print(f'Validation Loss: {avg_val_loss:.4f}')\n",
        "        print(f'Validation Accuracy: {100 * val_correct/val_total:.2f}%\\n')\n",
        "\n",
        "    # Load best model\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model\n",
        "\n",
        "def predict_email(model, tokenizer, text, device, label_classes):\n",
        "    model.eval()\n",
        "    encoding = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=256,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_attention_mask=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    input_ids = encoding['input_ids'].to(device)\n",
        "    attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        probabilities = torch.softmax(outputs, dim=1)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "    return {\n",
        "        'predicted_class': label_classes[predicted.item()],\n",
        "        'confidence': probabilities[0][predicted.item()].item()\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Load the dataset\n",
        "    df_main = pd.read_csv('/content/drive/MyDrive/spam_email_dataset.csv')  # Replace with your dataset path\n",
        "    df=df_main.head(200)\n",
        "\n",
        "    # Prepare data\n",
        "    df, label_classes = prepare_data(df)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        df['text'].values,\n",
        "        df['label'].values,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=df['label']\n",
        "    )\n",
        "\n",
        "    # Initialize tokenizer and model\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    model = SpamClassifier(num_classes=len(label_classes))\n",
        "\n",
        "    # Create datasets and dataloaders\n",
        "    train_dataset = EmailDataset(X_train, y_train, tokenizer)\n",
        "    val_dataset = EmailDataset(X_val, y_val, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=16)\n",
        "\n",
        "    # Train the model\n",
        "    model = train_model(model, train_loader, val_loader)\n",
        "\n",
        "    # Save the trained model\n",
        "    torch.save({\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'label_classes': label_classes\n",
        "    }, 'spam_classifier.pth')\n",
        "\n",
        "    # Example prediction\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    sample_email = \"Get rich quick! Buy now!\"\n",
        "    result = predict_email(model, tokenizer, sample_email, device, label_classes)\n",
        "    print(f\"Predicted category: {result['predicted_class']}\")\n",
        "    print(f\"Confidence: {result['confidence']:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwQaA1DD6t-m",
        "outputId": "b7c27a12-6b5f-4832-ad5a-61b352a06295"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-f9b6d884261a>:65: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].str.lower()\n",
            "<ipython-input-1-f9b6d884261a>:66: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].str.replace(r'\\s+', ' ', regex=True)\n",
            "<ipython-input-1-f9b6d884261a>:67: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text'] = df['text'].str.strip()\n",
            "<ipython-input-1-f9b6d884261a>:71: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['label'] = le.fit_transform(df['label'])\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1/5: 100%|██████████| 10/10 [09:07<00:00, 54.77s/it, loss=0.673, accuracy=45.6]\n",
            "Validation: 100%|██████████| 3/3 [00:36<00:00, 12.25s/it, loss=0.681, accuracy=70]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/5:\n",
            "Training Loss: 0.7048\n",
            "Training Accuracy: 45.62%\n",
            "Validation Loss: 0.6826\n",
            "Validation Accuracy: 70.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2/5: 100%|██████████| 10/10 [08:49<00:00, 52.93s/it, loss=0.638, accuracy=71.2]\n",
            "Validation: 100%|██████████| 3/3 [00:36<00:00, 12.26s/it, loss=0.634, accuracy=75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/5:\n",
            "Training Loss: 0.6650\n",
            "Training Accuracy: 71.25%\n",
            "Validation Loss: 0.6366\n",
            "Validation Accuracy: 75.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3/5: 100%|██████████| 10/10 [08:48<00:00, 52.90s/it, loss=0.568, accuracy=74.4]\n",
            "Validation: 100%|██████████| 3/3 [00:37<00:00, 12.45s/it, loss=0.561, accuracy=75]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/5:\n",
            "Training Loss: 0.6076\n",
            "Training Accuracy: 74.38%\n",
            "Validation Loss: 0.5738\n",
            "Validation Accuracy: 75.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4/5: 100%|██████████| 10/10 [08:53<00:00, 53.34s/it, loss=0.519, accuracy=85]\n",
            "Validation: 100%|██████████| 3/3 [00:36<00:00, 12.27s/it, loss=0.487, accuracy=80]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/5:\n",
            "Training Loss: 0.5460\n",
            "Training Accuracy: 85.00%\n",
            "Validation Loss: 0.5128\n",
            "Validation Accuracy: 80.00%\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5/5: 100%|██████████| 10/10 [08:50<00:00, 53.04s/it, loss=0.495, accuracy=92.5]\n",
            "Validation: 100%|██████████| 3/3 [00:38<00:00, 12.86s/it, loss=0.428, accuracy=95]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/5:\n",
            "Training Loss: 0.4788\n",
            "Training Accuracy: 92.50%\n",
            "Validation Loss: 0.4575\n",
            "Validation Accuracy: 95.00%\n",
            "\n",
            "Predicted category: spam\n",
            "Confidence: 0.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6aelUvZQCTw9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}